services:

  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.14.0
    container_name: mlflow-server
    ports:
      - "5000:5000"
    volumes:
      - ./mlruns:/mlflow/mlruns
      - ./artifacts:/mlflow/artifacts
    command: >
      mlflow server
      --host 0.0.0.0
      --port 5000
      --backend-store-uri /mlflow/mlruns
      --default-artifact-root /mlflow/artifacts
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import socket; s=socket.socket(); s.connect(('localhost', 5000)); s.close()\""]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 180s

  inference:
    build:
      context: .
      dockerfile: docker/Dockerfile.inference
    container_name: inference-service
    ports:
      - "8000:8000"
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - MODEL_URI=models:/rul-predictor/2
    depends_on:
      mlflow:
        condition: service_healthy
    volumes:
      - ./mlruns:/app/mlruns
    restart: on-failure

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    depends_on:
      - inference

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    depends_on:
      - prometheus